{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QlJpXyfNDKbw"
      },
      "outputs": [],
      "source": [
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "#!pip install \"unsloth[colab-new]\" huggingface transformers bitsandbytes\n",
        "#!pip install -v -U git+https://github.com/facebookresearch/xformers.git@main\n",
        "#!pip install trl peft datasets flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBhyKYVEDLOD",
        "outputId": "89e5bd4f-13b4-4007-97a1-1a36c68e97cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "321AAWy1F7zD"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048\n",
        "load_in_4bit = True\n",
        "fourbits_model = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POCi--LGLLpZ",
        "outputId": "d3ee6e5a-1f20-4e67-be00-6bb10e87a150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28+33a51bd.d20240816. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = fourbits_model,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPJKFBtIMIbT",
        "outputId": "99e2e681-f4a3-4419-d65a-ed9770ab8bcd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# Do model patching and add fast LoRA weights\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    max_seq_length = max_seq_length,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1IvgOfSO99d"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hfRluXR7POWR",
        "outputId": "94b83aa3-a38c-4d03-fc05-a35ab6de0b25"
      },
      "outputs": [],
      "source": [
        "dataset.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Human<human>: Describe the location where you find yourself in a serene scene inside of a medieval inn with a rice paper screen.\n",
        "\n",
        "Bot<bot>: Sturdy rectory tables and low benches offer a quiet place to eat and drink food brought with you or bought from the nearby bakery. At the back of the room, the door leading to the kitchen and the owner's private quarters is half-hidden by a rice paper screen.\",\n",
        "\n",
        "---\n",
        "Human<human>: I want to find out more about a certain type of bird.\n",
        "\n",
        "Bot<bot>: That sounds like a great idea. What kind of bird are you interested in?\",\n",
        "\n",
        "---\n",
        "Human<human>: round number to nearest integer\n",
        "\n",
        "Bot<bot>: Here is some code to round number 4.0005 up to 3 decimal places, round(4.0005, 3)\n",
        "\n",
        "---\n",
        "Human<human>: I don't understand my tax form. Can you help explain?\n",
        "\n",
        "Bot<bot>: I'm sorry, I'm not able to help with your tax form. It is best to consult a tax specialist or the IRS for further explanation.\","
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "22507e991d96403a90d48fad611cf2f2",
            "99db02f27d6c4867b17bdbc8d692ac98",
            "22f1bf8d8b5e4b7c8cedf1228e5aec91",
            "b44250103c044501a0bfd8f7b554bab7",
            "57d18908692c412d80b12fc7e6ca47dc",
            "5573358bcbf74c55ba05a79e849fd1f4",
            "56d78b810ff24623a5a819c0e359da5f",
            "b6e226bb2aef4bd2bf2f5f21af664ed1",
            "c63938a606c2486aa274ab21370dd8bb",
            "9ddb2486842d40789596124ee2957dd2",
            "9c3f2cb59fe04788a8f3111668ae98e9"
          ]
        },
        "id": "8WhM3QTCPh7V",
        "outputId": "fc73b30e-5c83-46bd-e91b-8ae3a8407f06"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    tokenizer = tokenizer,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 20,\n",
        "        max_steps = 120,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        learning_rate = 5e-5,\n",
        "        weight_decay = 0.01,\n",
        "        output_dir = \"outputs\",\n",
        "        optim = \"adamw_8bit\",\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wk59eBOMQ25R",
        "outputId": "8cff1c4d-8954-4135-9f8d-a80cb277c75b"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PeftModelForCausalLM\n",
        "\n",
        "    - (base_model): LoraModel\n",
        "\n",
        "        - (model): LlamaForCausalLM\n",
        "\n",
        "            - (model): LlamaModel()\n",
        "\n",
        "            - (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F9fm_kYWQmOv",
        "outputId": "39f41f35-c4c4-4b5b-b1f4-0d59795d8ea5"
      },
      "outputs": [],
      "source": [
        "#start training\n",
        "trainer.train()\n",
        "trainer.save_model(\"finetuned_llm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step\tTraining Loss\n",
        "\n",
        "1\t1.737900\n",
        "\n",
        "10\t1.051600\n",
        "\n",
        "20\t1.380700\n",
        "\n",
        "30\t1.025000\n",
        "\n",
        "40\t1.595500\n",
        "\n",
        "50\t1.414300\n",
        "\n",
        "60\t1.337900\n",
        "\n",
        "70\t1.288200\n",
        "\n",
        "80\t1.418700\n",
        "\n",
        "90\t1.016300\n",
        "\n",
        "100\t0.969700\n",
        "\n",
        "110\t1.292600\n",
        "\n",
        "120\t1.123400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
